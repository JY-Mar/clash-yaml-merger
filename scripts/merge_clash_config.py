#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Clashé…ç½®æ–‡ä»¶æ•´åˆå·¥å…·
ä»GitHubç§æœ‰ä»“åº“è¯»å–å¤šä¸ªè®¢é˜…YAMLæ–‡ä»¶ï¼Œæ•´åˆä»£ç†èŠ‚ç‚¹å’Œè§„åˆ™ï¼Œç”Ÿæˆç»Ÿä¸€çš„Clashé…ç½®
"""

import os
import re
import sys
import yaml
import requests
import base64
import json
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
import logging
from functools import reduce

script_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(script_dir)
sys.path.insert(0, root_dir)

from utils.files_utils import load_yaml_content
from utils.patterns import REMOTE_FILE_PATTERN, REMOTE_YAML_PATTERN, FCONFS_DIR_PATTERN
from utils.config_utils import load_config
from utils.merge_utils import deep_merge
from utils.string_utils import (
    cut_fonfs_name,
    desensitize_url,
    split_str_to_1d_array,
    split_str_to_2d_array,
)
from utils.array_utils import unshift_to_array, filter_valid_strings

# è®¾ç½®é»˜è®¤ç¼–ç 
import codecs

# å¼ºåˆ¶è®¾ç½®UTF-8ç¼–ç 
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["PYTHONUTF8"] = "1"

# ç¡®ä¿UTF-8ç¼–ç 
try:
    if hasattr(sys.stdout, "buffer") and sys.stdout.encoding != "utf-8":
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
    if hasattr(sys.stderr, "buffer") and sys.stderr.encoding != "utf-8":
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")
except:
    pass  # åœ¨æŸäº›ç¯å¢ƒä¸‹å¯èƒ½ä¼šå¤±è´¥ï¼Œå¿½ç•¥é”™è¯¯

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# settings.yaml é…ç½®
settings_config = load_config()


class ClashConfigMerger:
    def __init__(
        self,
        github_token: str = "",
        repo_owner: str = "",
        repo_name: str = "",
        local_mode: bool = False,
    ):
        """
        åˆå§‹åŒ–Clashé…ç½®åˆå¹¶å™¨

        Args:
            github_token: GitHubè®¿é—®ä»¤ç‰Œ
            repo_owner: ä»“åº“æ‰€æœ‰è€…
            repo_name: ä»“åº“åç§°
            local_mode: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å¼
        """
        self.local_mode = local_mode
        self.github_token = github_token
        self.repo_owner = repo_owner
        self.repo_name = repo_name

        if not local_mode:
            self.headers = {
                "Authorization": f"token {github_token}",
                "Accept": "application/vnd.github.v3+json",
            }
            self.base_url = (
                f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents"
            )

    def get_file_content(self, file_path: str) -> Optional[str]:
        """
        è·å–æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒæœ¬åœ°å’ŒGitHubæ¨¡å¼ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        if self.local_mode:
            # æœ¬åœ°æ¨¡å¼ï¼šç›´æ¥è¯»å–æ–‡ä»¶
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    logger.info(f"æˆåŠŸè¯»å–æœ¬åœ°æ–‡ä»¶: {file_path}")
                    return content
            except FileNotFoundError:
                logger.error(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
            except Exception as e:
                logger.error(f"è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                return None
        else:
            # GitHubæ¨¡å¼ï¼šé€šè¿‡APIè·å–
            try:
                if re.fullmatch(REMOTE_YAML_PATTERN, file_path) is not None:
                    # æ˜¯yamlæ–‡ä»¶è·¯å¾„ç›´æ¥è¯»å–
                    url = file_path
                    response = requests.get(url)
                    try:
                        yaml_raw_content = response.text
                    except json.JSONDecodeError as e:
                        yaml_raw_content = None
                        logger.error(f"è§£æå¤±è´¥ï¼šä¸æ˜¯åˆæ³•çš„ JSON æ ¼å¼: {e}")

                    if yaml_raw_content:
                        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶: {desensitize_url(file_path)}")

                    return yaml_raw_content
                else:
                    url = f"{self.base_url}/{file_path}"
                    response = requests.get(url, headers=self.headers)
                    response.raise_for_status()
                    file_data = response.json()

                if file_data["encoding"] == "base64":
                    content = base64.b64decode(file_data["content"]).decode("utf-8")
                    logger.info(f"æˆåŠŸè·å–æ–‡ä»¶: {desensitize_url(file_path)}")
                    return content
                else:
                    logger.error(f"ä¸æ”¯æŒçš„ç¼–ç æ ¼å¼: {file_data['encoding']}")
                    return None

            except requests.exceptions.RequestException as e:
                logger.error(f"è·å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                return None
            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                return None

    def get_directory_files(self, directory_path: str) -> List[str]:
        """
        è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæœ¬åœ°å’ŒGitHubæ¨¡å¼ï¼‰

        Args:
            directory_path: ç›®å½•è·¯å¾„

        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if self.local_mode:
            # æœ¬åœ°æ¨¡å¼ï¼šæ‰«ææœ¬åœ°ç›®å½•
            try:
                if not os.path.exists(directory_path):
                    logger.warning(f"æœ¬åœ°ç›®å½•ä¸å­˜åœ¨: {directory_path}")
                    return []

                file_paths = []
                for filename in os.listdir(directory_path):
                    if filename.endswith(".yaml") or filename.endswith(".yml"):
                        file_path = os.path.join(directory_path, filename)
                        file_paths.append(file_path)

                logger.info(
                    f"å‘ç° {len(file_paths)} ä¸ªYAMLæ–‡ä»¶åœ¨æœ¬åœ°ç›®å½•: {directory_path}"
                )
                return file_paths

            except Exception as e:
                logger.error(f"æ‰«ææœ¬åœ°ç›®å½•å¤±è´¥ {directory_path}: {e}")
                return []
        else:
            # GitHubæ¨¡å¼ï¼šé€šè¿‡APIè·å–
            try:
                url = f"{self.base_url}/{directory_path}"
                response = requests.get(url, headers=self.headers)
                response.raise_for_status()

                files = response.json()
                file_paths = []

                for file_info in files:
                    if file_info["type"] == "file" and file_info["name"].endswith(
                        ".yaml"
                    ):
                        file_paths.append(file_info["path"])

                if len(file_paths) == 0:
                    logger.warning(
                        f"å‘ç° {len(file_paths)} ä¸ªYAMLæ–‡ä»¶åœ¨ç›®å½•: {directory_path}"
                    )
                else:
                    logger.info(
                        f"å‘ç° {len(file_paths)} ä¸ªYAMLæ–‡ä»¶åœ¨ç›®å½•: {directory_path}"
                    )
                return file_paths

            except requests.exceptions.RequestException as e:
                logger.error(f"è·å–ç›®å½•æ–‡ä»¶åˆ—è¡¨å¤±è´¥ {directory_path}: {e}")
                return []

    def merge_proxies(self, configs_with_sources: List[tuple]) -> List[Dict[str, Any]]:
        """
        åˆå¹¶å¤šä¸ªé…ç½®æ–‡ä»¶çš„ä»£ç†èŠ‚ç‚¹

        Args:
            configs_with_sources: é…ç½®æ–‡ä»¶å’Œæ¥æºä¿¡æ¯çš„å…ƒç»„åˆ—è¡¨ [(config, source_file), ...]

        Returns:
            åˆå¹¶åçš„ä»£ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆåŒ…å«æ¥æºä¿¡æ¯ï¼‰
        """
        merged_proxies = []
        seen_names = set()

        for config, source_file in configs_with_sources:
            if "proxies" in config and isinstance(config["proxies"], list):
                source_name = os.path.basename(source_file).replace(".yaml", "")
                for proxy in config["proxies"]:
                    if isinstance(proxy, dict) and "name" in proxy:
                        # é¿å…é‡å¤çš„ä»£ç†èŠ‚ç‚¹åç§°
                        original_name = proxy["name"]
                        name = original_name
                        counter = 1

                        while name in seen_names:
                            name = f"{original_name}_{counter}"
                            counter += 1

                        proxy["name"] = name
                        proxy["_source_file"] = source_name  # æ·»åŠ æ¥æºæ ‡è¯†
                        seen_names.add(name)
                        merged_proxies.append(proxy)

        logger.info(f"åˆå¹¶äº† {len(merged_proxies)} ä¸ªä»£ç†èŠ‚ç‚¹")
        return merged_proxies

    def merge_rules(self, rules_files: List[str]) -> List[str]:
        """
        åˆå¹¶è§„åˆ™åˆ—è¡¨ï¼ˆåªä½¿ç”¨ruleç›®å½•ä¸‹çš„è§„åˆ™æ–‡ä»¶ï¼‰

        Args:
            rules_files: è§„åˆ™æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„è§„åˆ™åˆ—è¡¨
        """
        merged_rules = []
        seen_rules = set()

        # åªä»è§„åˆ™æ–‡ä»¶ä¸­åŠ è½½è§„åˆ™ï¼Œå¿½ç•¥ proxies æ–‡ä»¶ä¸­çš„è§„åˆ™
        for rule_file_path in rules_files:
            content = self.get_file_content(rule_file_path)
            if content:
                rule_data = load_yaml_content(content)
                logger.info(f"è§„åˆ™æ–‡ä»¶ {rule_file_path}")
                if rule_data and "payload" in rule_data:
                    rule_file_name = os.path.basename(rule_file_path).replace(
                        ".yaml", ""
                    )
                    logger.info(f"å¤„ç†è§„åˆ™æ–‡ä»¶: {rule_file_name}")

                    for rule in rule_data["payload"]:
                        if isinstance(rule, str) and rule not in seen_rules:
                            # ç¡®ä¿è§„åˆ™æ ¼å¼æ­£ç¡®ï¼Œæ‰€æœ‰è§„åˆ™éƒ½æŒ‡å‘"ç½‘ç»œä»£ç†"
                            rule = rule.strip()
                            if rule:
                                # æ‰€æœ‰è§„åˆ™éƒ½æŒ‡å‘"ç½‘ç»œä»£ç†"ç»„
                                formatted_rule = f"{rule},ç½‘ç»œä»£ç†"
                                merged_rules.append(formatted_rule)
                                seen_rules.add(formatted_rule)

        logger.info(f"åˆå¹¶äº† {len(merged_rules)} æ¡è§„åˆ™")
        return merged_rules

    def create_proxy_groups(
        self, proxies: List[Dict[str, Any]], proxies_files: List[str]
    ) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºç­–ç•¥ç»„ç»“æ„

        Args:
            proxies: ä»£ç†èŠ‚ç‚¹åˆ—è¡¨
            proxies_files: è®¢é˜…æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            ç­–ç•¥ç»„é…ç½®åˆ—è¡¨
        """
        proxy_names = [proxy["name"] for proxy in proxies if "name" in proxy]

        # æŒ‰è®¢é˜…æ–‡ä»¶åˆ†ç»„ä»£ç†èŠ‚ç‚¹ - åŸºäºæ¥æºä¿¡æ¯è¿›è¡Œç²¾ç¡®åˆ†ç»„
        temp_groups = {}
        for file_path in proxies_files:
            # ä»æ–‡ä»¶è·¯å¾„æå–æ–‡ä»¶åä½œä¸ºåˆ†ç»„å
            file_name = os.path.basename(file_path).replace(".yaml", "")
            temp_groups[file_name] = []

        # æ ¹æ®ä»£ç†çš„æ¥æºä¿¡æ¯è¿›è¡Œç²¾ç¡®åˆ†ç»„
        for proxy in proxies:
            if isinstance(proxy, dict) and "_source_file" in proxy:
                source_name = proxy["_source_file"]
                proxy_name = proxy.get("name", "")
                if source_name in temp_groups and proxy_name:
                    temp_groups[source_name].append(proxy_name)

        # åˆ›å»ºç­–ç•¥ç»„åˆ—è¡¨
        proxy_groups = []

        # 1. åˆ›å»ºä¸»ç½‘ç»œç­–ç•¥ç»„ï¼ˆåªåŒ…å« proxies åˆ†ç»„ï¼Œä¸åŒ…å« rules åˆ†ç»„ï¼‰
        temp_group_names = list(temp_groups.keys())

        network_proxy_options = ["è‡ªåŠ¨é€‰æ‹©", "æ•…éšœè½¬ç§»"] + temp_group_names
        proxy_groups.append(
            {"name": "ç½‘ç»œä»£ç†", "type": "select", "proxies": network_proxy_options}
        )

        # 2. åˆ›å»ºè‡ªåŠ¨é€‰æ‹©å’Œæ•…éšœè½¬ç§»ç»„
        proxy_groups.extend(
            [
                {
                    "name": "è‡ªåŠ¨é€‰æ‹©",
                    "type": "url-test",
                    "proxies": proxy_names,
                    "url": "http://www.gstatic.com/generate_204",
                    "interval": 300,
                },
                {
                    "name": "æ•…éšœè½¬ç§»",
                    "type": "fallback",
                    "proxies": proxy_names,
                    "url": "http://www.gstatic.com/generate_204",
                    "interval": 300,
                },
            ]
        )

        # 3. ä¸ºæ¯ä¸ªè®¢é˜…æ–‡ä»¶åˆ›å»ºç­–ç•¥ç»„ï¼ˆåªä¸º proxies æ–‡ä»¶åˆ›å»ºï¼Œä¸ä¸º rules æ–‡ä»¶åˆ›å»ºï¼‰
        for temp_item_name, temp_item_proxies in temp_groups.items():
            if temp_item_proxies:
                proxy_groups.append(
                    {
                        "name": temp_item_name,
                        "type": "select",
                        "proxies": ["è‡ªåŠ¨é€‰æ‹©", "æ•…éšœè½¬ç§»"] + temp_item_proxies,
                    }
                )

        logger.info(f"åˆ›å»ºäº† {len(proxy_groups)} ä¸ªç­–ç•¥ç»„")
        return proxy_groups

    def create_base_config(self) -> Dict[str, Any]:
        """
        åˆ›å»ºåŸºç¡€é…ç½®

        Returns:
            åŸºç¡€é…ç½®å­—å…¸
        """
        return {
            # HTTP ä»£ç†ç«¯å£
            "port": 7890,
            # SOCKS5 ä»£ç†ç«¯å£
            "socks-port": 7891,
            # HTTP(S) and SOCKS5 å…±ç”¨ç«¯å£
            "mixed-port": 7892,
            # Linux å’Œ macOS çš„ redir é€æ˜ä»£ç†ç«¯å£ (é‡å®šå‘ TCP å’Œ TProxy UDP æµé‡)
            "redir-port": 7893,
            # Linux çš„é€æ˜ä»£ç†ç«¯å£ï¼ˆé€‚ç”¨äº TProxy TCP å’Œ TProxy UDP æµé‡)
            "tproxy-port": 7894,
            "unified-delay": True,
            "tcp-concurrent": True,
            "find-process-mode": "strict",
            # å…è®¸å±€åŸŸç½‘çš„è¿æ¥ï¼ˆå¯ç”¨æ¥å…±äº«ä»£ç†ï¼‰
            "allow-lan": True,
            # Clash è·¯ç”±å·¥ä½œæ¨¡å¼
            # è§„åˆ™æ¨¡å¼ï¼šruleï¼ˆè§„åˆ™ï¼‰ / globalï¼ˆå…¨å±€ä»£ç†ï¼‰/ directï¼ˆå…¨å±€ç›´è¿ï¼‰
            "mode": "rule",
            # Clash é»˜è®¤å°†æ—¥å¿—è¾“å‡ºè‡³ STDOUT
            # è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ« (é»˜è®¤çº§åˆ«ï¼šsilentï¼Œå³ä¸è¾“å‡ºä»»ä½•å†…å®¹ï¼Œä»¥é¿å…å› æ—¥å¿—å†…å®¹è¿‡å¤§è€Œå¯¼è‡´ç¨‹åºå†…å­˜æº¢å‡ºï¼‰ã€‚
            # 5 ä¸ªçº§åˆ«ï¼šsilent / info / warning / error / debugã€‚çº§åˆ«è¶Šé«˜æ—¥å¿—è¾“å‡ºé‡è¶Šå¤§ï¼Œè¶Šå€¾å‘äºè°ƒè¯•ï¼Œè‹¥éœ€è¦è¯·è‡ªè¡Œå¼€å¯ã€‚
            "log-level": "silent",
            "ipv6": True,
            "udp": True,
            "bind-address": "*",
            # clash çš„ RESTful API ç›‘å¬åœ°å€
            "external-controller": "0.0.0.0:9090",
            "external-controller-tls": "0.0.0.0:9443",
            "external-controller-unix": "mihomo.sock",
            "external-controller-pipe": "\\.\\pipe\\mihomo",
            # å­˜æ”¾é…ç½®æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼Œæˆ–å­˜æ”¾ç½‘é¡µé™æ€èµ„æºçš„ç»å¯¹è·¯å¾„
            # Clash core å°†ä¼šå°†å…¶éƒ¨ç½²åœ¨ http://{{external-controller}}/ui
            "external-ui": "ui",
            "external-ui-name": "zashboard",
            "external-ui-url": "https://github.com/Zephyruso/zashboard/archive/refs/heads/gh-pages.zip",
            "external-doh-server": "/dns-query",
            "global-client-fingerprint": "chrome",
            "dns": {
                "enable": True,
                "ipv6": False,
                "default-nameserver": ["223.5.5.5", "119.29.29.29", "114.114.114.114"],
                "enhanced-mode": "fake-ip",
                "fake-ip-range": "198.18.0.1/16",
                "use-hosts": True,
                "nameserver": ["223.5.5.5", "119.29.29.29", "114.114.114.114"],
                "fallback": ["1.1.1.1", "8.8.8.8"],
                "fallback-filter": {
                    "geoip": True,
                    "geoip-code": "CN",
                    "ipcidr": ["240.0.0.0/4"],
                },
            },
        }

    def generate_merged_config(
        self,
        fconfs_directories: List[str] = ["fconfs"],
        proxy_providers_directory: str = "proxy-providers",
        proxies_directory: str = "proxies",
        rule_providers_directory: str = "rule-providers",
        rules_directory: str = "rules",
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆåˆå¹¶åçš„é…ç½®æ–‡ä»¶

        Args:
            fconfs_directories: å…¨é‡é…ç½®æ–‡ä»¶ç›®å½•ï¼Œæ”¯æŒç§æœ‰ä»“åº“ç›®å½•ã€å•ç‹¬æŒ‡å®šçš„yamlæ–‡ä»¶
            proxy_providers_directory: ä»£ç†é›†æ–‡ä»¶ç›®å½•
            proxies_directory: ä»£ç†èŠ‚ç‚¹æ–‡ä»¶ç›®å½•
            rule_providers_directory: è§„åˆ™é›†æ–‡ä»¶ç›®å½•
            rules_directory: è§„åˆ™æ–‡ä»¶ç›®å½•

        Returns:
            åˆå¹¶åçš„åŸºç¡€é…ç½®
        """

        # 1. åˆ›å»ºåŸºç¡€é…ç½®
        merged_config = self.create_base_config()

        # MARK: 2.1 å…¨é‡é…ç½®
        # 2.1.1 è·å–å…¨é‡é…ç½®æ–‡ä»¶åˆ—è¡¨
        fconfs_files: List[str] = []
        if fconfs_directories:
            for fconf_directory in fconfs_directories:
                if re.fullmatch(REMOTE_YAML_PATTERN, fconf_directory) is not None:
                    fconfs_files.extend([fconf_directory])
                else:
                    fconfs_files.extend(self.get_directory_files(fconf_directory))
        if not fconfs_files:
            logger.warning(f"æœªæ‰¾åˆ°å…¨é‡é…ç½®æ–‡ä»¶åœ¨ç›®å½•: {fconfs_directories}")

        # 2.1.2 ä»å…¨é‡é…ç½®æ–‡ä»¶åˆ—è¡¨åŠ è½½æ‰€æœ‰å…¨é‡é…ç½®
        configs_from_fconf_files: List[Dict[str, Any]] = []
        for file_path in fconfs_files:
            content = self.get_file_content(file_path)
            if content:
                config = load_yaml_content(content)
                if config:
                    configs_from_fconf_files.append((config))

        if not configs_from_fconf_files:
            logger.error(f"æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„å…¨é‡é…ç½®æ–‡ä»¶")
            return {}

        # 2.1.3 åˆå¹¶å…¨é‡é…ç½®
        if configs_from_fconf_files:
            merged_config = reduce(deep_merge, configs_from_fconf_files)

        # MARK: 2.2 ä»£ç†é›†
        # 2.2.1 è·å–ä»£ç†é›†æ–‡ä»¶åˆ—è¡¨
        proxy_providers_files = self.get_directory_files(proxy_providers_directory)
        if not proxy_providers_files:
            logger.warning(f"æœªæ‰¾åˆ°ä»£ç†é›†æ–‡ä»¶åœ¨ç›®å½•: {proxy_providers_directory}")

        # 2.2.2 åŠ è½½æ‰€æœ‰ä»£ç†é›†é…ç½®
        configs_from_proxy_providers_files = []
        for file_path in proxy_providers_files:
            content = self.get_file_content(file_path)
            if content:
                config = load_yaml_content(content)
                if config:
                    configs_from_proxy_providers_files.append((config))

        if not configs_from_proxy_providers_files:
            logger.error(f"æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ä»£ç†èŠ‚ç‚¹é…ç½®æ–‡ä»¶")
            # return {}

        # 2.2.3 åˆå¹¶ä»£ç†é›†
        if configs_from_proxy_providers_files:
            merged_proxy_providers = reduce(
                deep_merge, configs_from_proxy_providers_files
            )
            merged_config["proxy-providers"] = merged_proxy_providers

        # MARK: 2.3 ä»£ç†èŠ‚ç‚¹
        # 2.3.1 è·å–ä»£ç†èŠ‚ç‚¹æ–‡ä»¶åˆ—è¡¨
        proxies_files = self.get_directory_files(proxies_directory)
        if not proxies_files:
            logger.warning(f"æœªæ‰¾åˆ°ä»£ç†èŠ‚ç‚¹æ–‡ä»¶åœ¨ç›®å½•: {proxies_directory}")

        # 2.3.2 åŠ è½½æ‰€æœ‰ä»£ç†èŠ‚ç‚¹é…ç½®
        configs_from_proxies_files = []
        for file_path in proxies_files:
            content = self.get_file_content(file_path)
            if content:
                config = load_yaml_content(content)
                if config:
                    configs_from_proxies_files.append((config))

        if not configs_from_proxies_files:
            logger.error(f"æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ä»£ç†èŠ‚ç‚¹é…ç½®æ–‡ä»¶")
            # return {}

        # 2.3.3 åˆå¹¶ä»£ç†èŠ‚ç‚¹
        if configs_from_proxies_files:
            merged_proxies = reduce(deep_merge, configs_from_proxies_files)
            merged_config["proxies"] = merged_proxies

        # MARK: 2.4 è§„åˆ™é›†
        # 2.4.1 è·å–è§„åˆ™é›†æ–‡ä»¶åˆ—è¡¨
        rule_providers_files = self.get_directory_files(rule_providers_directory)
        if not rule_providers_files:
            logger.warning(f"æœªæ‰¾åˆ°è§„åˆ™é›†æ–‡ä»¶åœ¨ç›®å½•: {rule_providers_directory}")

        # 2.4.2 åŠ è½½æ‰€æœ‰è§„åˆ™é›†é…ç½®
        configs_from_rule_providers_files = []
        for file_path in rule_providers_files:
            content = self.get_file_content(file_path)
            if content:
                config = load_yaml_content(content)
                if config:
                    configs_from_rule_providers_files.append((config))

        if not configs_from_rule_providers_files:
            logger.error(f"æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è§„åˆ™é›†é…ç½®æ–‡ä»¶")
            # return {}

        # 2.4.3 åˆå¹¶è§„åˆ™é›†
        if configs_from_rule_providers_files:
            merged_rule_providers = reduce(
                deep_merge, configs_from_rule_providers_files
            )
            merged_config["rule-providers"] = merged_rule_providers

        # MARK: 2.5 è§„åˆ™
        # 2.5.1 è·å–è§„åˆ™æ–‡ä»¶åˆ—è¡¨
        rules_files = self.get_directory_files(rules_directory)
        if not rules_files:
            logger.warning(f"æœªæ‰¾åˆ°è§„åˆ™æ–‡ä»¶åœ¨ç›®å½•: {rules_directory}")

        # 2.5.2 åŠ è½½æ‰€æœ‰è§„åˆ™é…ç½®
        configs_from_rules_files = []
        for file_path in proxies_files:
            content = self.get_file_content(file_path)
            if content:
                config = load_yaml_content(content)
                if config:
                    configs_from_rules_files.append((config))

        if not configs_from_rules_files:
            logger.error(f"æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è§„åˆ™é…ç½®æ–‡ä»¶")
            # return {}

        # 2.5.3 åˆå¹¶è§„åˆ™
        if configs_from_rules_files:
            merged_rules = reduce(deep_merge, configs_from_rules_files)
            merged_config["rules"] = merged_rules

        # 3. æ¸…ç†ä»£ç†èŠ‚ç‚¹ä¸­çš„ä¸´æ—¶å­—æ®µ
        try:
            for proxy in merged_config.get("proxies", []):
                if isinstance(proxy, dict) and "_source_file" in proxy:
                    del proxy["_source_file"]
        except Exception as e:
            logger.error(f"æ¸…ç†ä»£ç†èŠ‚ç‚¹ä¸­çš„ä¸´æ—¶å­—æ®µå¤±è´¥: {e}")

        return merged_config

    def save_config_to_file(self, config: Dict[str, Any], output_path: str) -> bool:
        """
        ä¿å­˜é…ç½®åˆ°æ–‡ä»¶

        Args:
            config: é…ç½®å­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            # ä½¿ç”¨UTF-8 BOMç¼–ç å†™å…¥æ–‡ä»¶ï¼Œç¡®ä¿GitHub Pagesæ­£ç¡®è¯†åˆ«
            with open(output_path, "w", encoding="utf-8-sig", newline="\n") as f:
                # ä½¿ç”¨è‡ªå®šä¹‰çš„YAMLè¾“å‡ºæ ¼å¼ï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                yaml_content = yaml.dump(
                    config,
                    default_flow_style=False,
                    allow_unicode=True,
                    sort_keys=False,
                    encoding=None,  # è¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯å­—èŠ‚
                    width=1000,  # é¿å…é•¿è¡Œè¢«æŠ˜æ–­
                    indent=2,
                )

                # æ’å…¥å¤´éƒ¨å†…å®¹
                header = [
                    "# Automatically generated `Clash` yaml file",
                    "# Do not modify manually",
                    f"# Last Update: {datetime.now(timezone.utc).isoformat()}",
                ]
                # åˆå¹¶ä¸ºæœ€ç»ˆæ–‡æœ¬
                final_yaml_content = "\n".join(header) + "\n\n" + yaml_content

                # ç¡®ä¿å†™å…¥UTF-8ç¼–ç çš„å†…å®¹
                f.write(final_yaml_content)

            logger.info(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False


class ClashConfigInitParams:
    def __init__(
        self,
        local_mode: bool = False,
        merger: ClashConfigMerger | None = None,
        auth_token: str = "",
        output_dir: str = "",
        fconfs_dirs: List[List[str]] = [],
        fconfs_filenames: List[str] = [],
        proxy_providers_dir: str = "",
        proxies_dir: str = "",
        rule_providers_dir: str = "",
        rules_dir: str = "",
    ):
        """
        åˆå§‹åŒ–Clashé…ç½®åˆå§‹åŒ–å‚æ•°

        Args:
            local_mode: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å¼
            merger: Clashé…ç½®åˆå¹¶å¯¹è±¡
            auth_token: ç”¨æˆ·é‰´æƒä»¤ç‰Œ
            output_dir: è¾“å‡ºç›®å½•
            fconfs_dirs: å…¨é‡é…ç½®ç›®å½•åˆ—è¡¨
            fconfs_filenames: ç”Ÿæˆé…ç½®æ–‡ä»¶ååˆ—è¡¨
            proxy_providers_dir: ä»£ç†é›†ç›®å½•
            proxies_dir: ä»£ç†èŠ‚ç‚¹ç›®å½•
            rule_providers_dir: è§„åˆ™é›†ç›®å½•
            rules_dir: è§„åˆ™ç›®å½•
        """
        self.local_mode = local_mode
        self.merger = merger
        self.output_dir = output_dir
        self.fconfs_dirs = fconfs_dirs
        self.fconfs_filenames = fconfs_filenames
        self.proxy_providers_dir = proxy_providers_dir
        self.proxies_dir = proxies_dir
        self.rule_providers_dir = rule_providers_dir
        self.rules_dir = rules_dir
        self.auth_token = auth_token


def merger_init() -> ClashConfigInitParams:
    """
    åˆå§‹åŒ–mergerã€output_dirã€fconfs_dirsã€fconfs_filenamesã€proxy_providers_dirã€proxies_dirã€rule_providers_dirã€rules_dirã€auth_tokenç­‰é‡è¦å‚æ•°

    Returns:
        åˆå§‹åŒ–å‚æ•°å¯¹è±¡
    """

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æµ‹è¯•æ¨¡å¼
    local_mode = len(sys.argv) > 1 and sys.argv[1] == "--local"

    fconfs_dirs = [["fconfs"]]
    fconfs_filenames = ["filename"]
    proxy_providers_dir = "proxy-providers"
    proxies_dir = "proxies"
    rule_providers_dir = "rules-providers"
    rules_dir = "rules"

    if local_mode:
        logger.info(f"ğŸ§ª æœ¬åœ°æµ‹è¯•æ¨¡å¼")
        # æœ¬åœ°æ¨¡å¼é…ç½®
        merger = ClashConfigMerger(local_mode=True)
        output_dir = "output"
        auth_token = "local-test"
    else:
        logger.info(f"â˜ï¸ GitHubæ¨¡å¼")
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        github_token = os.getenv("GITHUB_TOKEN")
        repo_owner = os.getenv("REPO_OWNER", "your-username")
        repo_name = os.getenv("REPO_NAME", "clash-config")
        output_dir = os.getenv("OUTPUT_DIR", "docs")
        auth_token = os.getenv("AUTH_TOKEN", "default-token")
        _fconfs_remote_yamls = filter_valid_strings(
            [
                os.getenv("REMOTE_YAMLS", ""),
                settings_config["github"]["fconfs_remote_yamls"],
            ]
        )
        fconfs_remote_yamls = (
            ",".join(_fconfs_remote_yamls) if _fconfs_remote_yamls else ""
        )
        _fconfs_directories = filter_valid_strings(
            [
                os.getenv("FCONFS_DIRECTORIES", ""),
                settings_config["github"]["fconfs_directories"],
            ]
        )
        fconfs_directories = (
            ";".join(_fconfs_directories) if _fconfs_directories else ""
        )
        proxy_providers_directory = settings_config["github"][
            "proxy_providers_directory"
        ]
        proxies_directory = settings_config["github"]["proxies_directory"]
        rule_providers_directory = settings_config["github"]["rule_providers_directory"]
        rules_directory = settings_config["github"]["rules_directory"]

        fconfs_remote_yamls_1d_list: List[str] = []
        if fconfs_remote_yamls and isinstance(fconfs_remote_yamls, str):
            fconfs_remote_yamls_1d_list = split_str_to_1d_array(
                fconfs_remote_yamls.strip()
            )

        if fconfs_directories and isinstance(fconfs_directories, str):
            fconfs_directories_2d_list = split_str_to_2d_array(
                re.sub(FCONFS_DIR_PATTERN, r"\2", fconfs_directories)
            )
            # è·å–æ–‡ä»¶åï¼Œé»˜è®¤å–â€œname|ã€‚ã€‚ã€‚â€çš„nameå€¼ï¼Œå¦åˆ™å–ç¬¬ä¸€ä¸ªç›®å½•å
            fconfs_filenames = list(
                map(lambda f_d: cut_fonfs_name(f_d), fconfs_directories_2d_list)
            )
            fconfs_dirs = unshift_to_array(
                fconfs_directories_2d_list, fconfs_remote_yamls_1d_list
            )

        if proxy_providers_directory and isinstance(proxy_providers_directory, str):
            proxy_providers_dir = proxy_providers_directory.strip()

        if proxies_directory and isinstance(proxies_directory, str):
            proxies_dir = proxies_directory.strip()

        if rule_providers_directory and isinstance(rule_providers_directory, str):
            rule_providers_dir = rule_providers_directory.strip()

        if rules_directory and isinstance(rules_directory, str):
            rules_dir = rules_directory.strip()

        if not github_token:
            logger.error(f"æœªè®¾ç½®GITHUB_TOKENç¯å¢ƒå˜é‡")
            sys.exit(1)

        # åˆ›å»ºåˆå¹¶å™¨å®ä¾‹
        merger = ClashConfigMerger(
            github_token, repo_owner, repo_name, local_mode=False
        )

    return ClashConfigInitParams(
        local_mode=local_mode,
        merger=merger,
        output_dir=output_dir,
        fconfs_dirs=fconfs_dirs,
        fconfs_filenames=fconfs_filenames,
        proxy_providers_dir=proxy_providers_dir,
        proxies_dir=proxies_dir,
        rule_providers_dir=rule_providers_dir,
        rules_dir=rules_dir,
        auth_token=auth_token,
    )


def merger_gen_config():
    """
    ç”Ÿæˆåˆå¹¶åçš„é…ç½®
    """

    ida = merger_init()
    merged_configs: Dict[str, Dict[str, Any]] = {}
    if (
        ida.merger is not None
        and ida.fconfs_dirs
        and isinstance(ida.fconfs_dirs, list)
        and ida.fconfs_filenames
        and isinstance(ida.fconfs_filenames, list)
        and len(ida.fconfs_dirs) == len(ida.fconfs_filenames)
    ):
        _merger = ida.merger
        logger.info(f"=== â†“â†“â†“ å¼€å§‹ç”Ÿæˆåˆå¹¶é…ç½® â†“â†“â†“ ===")
        for i, dirs in enumerate(ida.fconfs_dirs):
            attr = ida.fconfs_filenames[i]
            dirs_desensitize = list(map(lambda dir: desensitize_url(dir), dirs))
            logger.info(
                f"=== [{i + 1} / {len(ida.fconfs_dirs)}] å¼€å§‹åˆå¹¶ {attr} <== {dirs_desensitize} ==="
            )
            merged_configs[attr] = _merger.generate_merged_config(
                dirs,
                ida.proxy_providers_dir,
                ida.proxies_dir,
                ida.rule_providers_dir,
                ida.rules_dir,
            )
            logger.info(f"=== [{i + 1} / {len(ida.fconfs_dirs)}] åˆå¹¶å®Œæˆ {attr}  ===")
        logger.info(f"=== â†‘â†‘â†‘ é…ç½®åˆå¹¶å®Œæˆ â†‘â†‘â†‘ ===")
    else:
        merged_configs = {}

    if not merged_configs:
        logger.error(f"ç”Ÿæˆé…ç½®å¤±è´¥")
        sys.exit(1)

    for filename, merged_config in merged_configs.items():
        if filename and merged_config:
            final_filename = f"-{filename}"
            # ä½¿ç”¨tokenä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†è¿›è¡Œè®¤è¯
            config_filename = f"{settings_config['output']['config_filename']}{final_filename}-{ida.auth_token}.yaml"
            output_path = os.path.join(ida.output_dir, config_filename)
            if not ida.merger or (
                ida.merger
                and not ida.merger.save_config_to_file(merged_config, output_path)
            ):
                sys.exit(1)

            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            now_date_formatted = datetime.now(timezone.utc).isoformat()
            stats = {
                "generated_at": now_date_formatted,
                # proxy-providers ä¸ªæ•°
                "proxy_providers_count": 0,
                # å• proxy-providers ä¸­æ‰€åŒ…å«çš„ proxies ä¸ªæ•°
                "proxy_providers_proxies_count": {},
                # å• proxies ä¸ªæ•°
                "proxies_count": 0,
                # proxy-groups ä¸ªæ•°
                "proxy_groups_count": 0,
                # è½½å…¥çš„ rule-providers ä¸ªæ•°
                "rule_providers_count": 0,
                # ä½¿ç”¨çš„ rule-providers ä¸ªæ•°
                "rule_providers_used_count": 0,
                # rules ä¸ªæ•°
                "rules_count": 0,
            }
            try:
                # proxy-providers
                _proxy_providers = merged_config.get("proxy-providers", {})
                proxy_providers_count = len(_proxy_providers)
                proxy_providers_proxies_count = {}
                for proxyProviderKey, proxyProviderValue in _proxy_providers.items():
                    proxyProviderUrl = proxyProviderValue.get("url", "")
                    if proxyProviderUrl and isinstance(proxyProviderUrl, str) and re.fullmatch(REMOTE_FILE_PATTERN, proxyProviderUrl) is not None:
                        _response = requests.get(proxyProviderUrl)
                        if _response.status_code == 200:
                            file_content = _response.text
                            yaml_content = load_yaml_content(file_content)
                            if yaml_content and isinstance(yaml_content, dict):
                                _proxies = yaml_content.get("proxies", [])
                                proxy_providers_proxies_count.update({ proxyProviderKey: len(_proxies) })
                            else:
                                proxy_providers_proxies_count.update({ proxyProviderKey: 0 })
                        else:
                            proxy_providers_proxies_count.update({ proxyProviderKey: 0 })
                    else:
                        proxy_providers_proxies_count.update({ proxyProviderKey: 0 })

                # proxies
                _proxies = merged_config.get("proxies", [])
                proxies_count = len(_proxies)
                # proxy-groups
                _proxy_groups = merged_config.get("proxy-groups", [])
                proxy_groups_count = len(_proxy_groups)
                # rule-providers
                _rule_providers = merged_config.get("rule-providers", {})
                rule_providers_count = len(_rule_providers)
                # rules
                _rules = merged_config.get("rules", [])
                rule_providers_used_count = len(
                    [s for s in _rules if isinstance(s, str) and s.strip().startswith("RULE-SET,")]
                )
                rules_count = len(_rules)
                stats.update(
                    {
                        "proxy_providers_count": proxy_providers_count,
                        "proxy_providers_proxies_count": proxy_providers_proxies_count,
                        "proxies_count": proxies_count,
                        "proxy_groups_count": proxy_groups_count,
                        "rule_providers_count": rule_providers_count,
                        "rule_providers_used_count": rule_providers_used_count,
                        "rules_count": rules_count,
                    }
                )
            except Exception as e:
                logger.error(f"ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

            stats_path = os.path.join(
                ida.output_dir,
                f"{settings_config['output']['stats_filename']}{final_filename}.json",
            )

            try:
                os.makedirs(ida.output_dir, exist_ok=True)
                with open(stats_path, "w", encoding="utf-8") as f:
                    json.dump(stats, f, indent=2, ensure_ascii=False)

                logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")
            except Exception as e:
                logger.warning(f"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

            logger.info(
                f"âœ… ä»»åŠ¡å®Œæˆ! ä»£ç†é›†: {stats['proxy_providers_count']}, ä»£ç†èŠ‚ç‚¹: {stats['proxies_count']}, è§„åˆ™: {stats['rule_providers_count']}, è§„åˆ™: {stats['rules_count']}"
            )
            logger.info(
                f"ğŸ“ é…ç½®æ–‡ä»¶: {settings_config['output']['config_filename']}{final_filename}-{{your-token}}.yaml"
            )
            if ida.local_mode:
                logger.info(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    merger_gen_config()


if __name__ == "__main__":
    main()
